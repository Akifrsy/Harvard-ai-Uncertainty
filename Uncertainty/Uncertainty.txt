Unconditional Probability: Degree of belief in a proposition in the absence of any other evidence.

Random Variable: Avariable in probability theory with a domain of possible values it can take on.

Roll
{1,2,3,4,5,6}

Weather
{sun, cloud, rain, wind,snow}

Traffic
{none, light, heavy}

Flight
{on time, delayed, cancelled}

Probability Distribution: 
    P(Flight = on time) = 0.6
    P(Flight = delayed) = 0.3
    P(Flight = cancelled) = 0.1

    P(Flight) = <0.6, 0.3, 0.1>

Independence: The knowledge that one event occurs does not affect the probability of the other event

Given clouds in the morning, what's the probability of rain in the afteroon?

    -80% of rainy afternoons start with cloudy mornings.
    -40% of days  have cloudy mornings.
    -10% of days have rainy afternoons.

Knowing 
    P(visible effect | unkown cause)

We can calculate
    P(unkown cause | visible effect)

Bayesian Network: Data structure that represents the dependencies among random variables.
    -Directed graph
    -Each node represents a random variable 
    -Arrow from X to Y means X is parent of Y
    -Each node X has probability distribution P(X | Parents(X))

                                        Rain 
                                {none, light, heavy}
                                    
                                     Maintenance
                                      {yes,no}

                                        Train
                                  {on time, delayed}

                                    Appointment
                                   {attend, miss}

Inference:
    -Query X: Variable for which to compute distribution.
    -Evidence variables E: Observed variables for event e.
    -Hidden variables Y: Non-Evidence, non-query variable.

    -Goal: Calculate P(X | e)

Inference by Enumeration
    X is the query variable.
    e is the evidence.
    y ranges over values of hidden variables.
    a normalizes the result.

Likelihood Weighting
    -Start by fixing the values for evidencce variables.
    -Sample the non-evidence variables using conditional probabilities in the Bayesian Network.
    -Weight each sample by its likelihood: the probability of all of the evidence.

P(Rain = light | Train = on time) ?

Markove Assumption: The assumption that the current state depends on only a finite fixed number of previous states.

Markove Chain: A sequence of random variables where the distribution of each variable follows the Markov assumption.

Hidden Markov Model: A Markov model for a system with hidden states that generate some observed event.

Sensor Markov Assumption: The assumption that the evidence variable depends only the corresponding state.



Task |                                                 |Definition|
Filtering ||||||||||||||| Given observations from start until now, calculate distribution for current state.
Prediction||||||||||||||| Given observations from start until now, calculate distribution for a future state.
Smoothing ||||||||||||||| Given observations from start until now, calculate distribution for past state.
Most Likely Explanation | Given observations from start until now, calculate most likely sequence of states.